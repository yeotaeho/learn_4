"""RAG ì„œë¹„ìŠ¤ - LangChain RAG ì²´ì¸ ê´€ë¦¬."""

import asyncio
import os
from pathlib import Path
from typing import Any, Optional

# QLoRA ê´€ë ¨ importëŠ” ì¡°ê±´ë¶€ë¡œ ì²˜ë¦¬ (OpenAI ì‚¬ìš© ì‹œ ë¶ˆí•„ìš”)
try:
    import torch
    from datasets import Dataset
    from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training
    from transformers import (
        AutoModelForCausalLM,
        AutoTokenizer,
        BitsAndBytesConfig,
        TrainingArguments,
        Trainer,
        DataCollatorForLanguageModeling,
    )
    QLORA_AVAILABLE = True
except ImportError:
    QLORA_AVAILABLE = False
    # QLoRA ê´€ë ¨ íƒ€ì… íŒíŠ¸ë¥¼ ìœ„í•œ ë”ë¯¸ í´ë˜ìŠ¤
    torch = None  # type: ignore
    Dataset = None  # type: ignore
    LoraConfig = None  # type: ignore
    PeftModel = None  # type: ignore
    get_peft_model = None  # type: ignore
    prepare_model_for_kbit_training = None  # type: ignore
    AutoModelForCausalLM = None  # type: ignore
    AutoTokenizer = None  # type: ignore
    BitsAndBytesConfig = None  # type: ignore
    TrainingArguments = None  # type: ignore
    Trainer = None  # type: ignore
    DataCollatorForLanguageModeling = None  # type: ignore

from langchain_community.vectorstores import PGVector
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough

from ..core.config import settings
from ..core.deps import get_llm


class RAGService:
    """RAG(Retrieval-Augmented Generation) ì„œë¹„ìŠ¤ í´ë˜ìŠ¤.

    ë¡œì»¬ LLM ë˜ëŠ” OpenAIë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ê¸°ë°˜ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    """

    # RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ë¡œì»¬ ëª¨ë¸ìš©ìœ¼ë¡œ ìµœì í™”) - ì¶”í›„ ì‚¬ìš© ì˜ˆì •
    # RAG_TEMPLATE = """ë‹¤ìŒ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
    # ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
    # ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
    #
    # ë¬¸ë§¥:
    # {context}
    #
    # ì§ˆë¬¸: {question}
    #
    # ë‹µë³€:"""

    def __init__(self, vectorstore: PGVector, llm: Any = None) -> None:
        """RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”.

        Args:
            vectorstore: PGVector ë²¡í„°ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤
            llm: LangChain í˜¸í™˜ LLM ì¸ìŠ¤í„´ìŠ¤ (Noneì´ë©´ ìë™ ë¡œë“œ)
        """
        self.vectorstore = vectorstore
        self.llm = llm or get_llm()
        self.chain = self._create_chain()

    # ì¶”í›„ ì‚¬ìš© ì˜ˆì • - í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
    # def _create_prompt(self) -> PromptTemplate:
    #     """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±."""
    #     return PromptTemplate(
    #         template=self.RAG_TEMPLATE,
    #         input_variables=["context", "question"],
    #     )

    # ì¶”í›„ ì‚¬ìš© ì˜ˆì • - ë¬¸ì„œ í¬ë§·íŒ…
    # @staticmethod
    # def _format_docs(docs: list) -> str:
    #     """ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜.
    #
    #     Args:
    #         docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    #
    #     Returns:
    #         í¬ë§·ëœ ë¬¸ì„œ ë¬¸ìì—´
    #     """
    #     if not docs:
    #         return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    #     return "\n\n".join(doc.page_content for doc in docs)

    def _create_chain(self):
        """ì²´ì¸ ìƒì„± - í˜„ì¬ëŠ” í”„ë¡¬í”„íŠ¸ì™€ ë¬¸ë§¥ ì—†ì´ ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ ì „ë‹¬."""
        # ì¶”í›„ ì‚¬ìš© ì˜ˆì • - RAG ì²´ì¸ (í”„ë¡¬í”„íŠ¸ì™€ ë¬¸ë§¥ í¬í•¨)
        # retriever = self.vectorstore.as_retriever(
        #     search_type="similarity",
        #     search_kwargs={"k": 3}
        # )
        #
        # prompt = self._create_prompt()
        #
        # chain = (
        #     {
        #         "context": retriever | self._format_docs,
        #         "question": RunnablePassthrough(),
        #     }
        #     | prompt
        #     | self.llm
        #     | StrOutputParser()
        # )

        # í˜„ì¬: í”„ë¡¬í”„íŠ¸ì™€ ë¬¸ë§¥ ì—†ì´ ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ LLMì— ì „ë‹¬
        chain = (
            RunnablePassthrough()
            | self.llm
            | StrOutputParser()
        )

        return chain

    def chat(self, message: str) -> str:
        """ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ RAG ì‘ë‹µ ìƒì„±.

        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€

        Returns:
            RAG ê¸°ë°˜ ì‘ë‹µ ë¬¸ìì—´
        """
        return self.chain.invoke(message)

    async def achat(self, message: str) -> str:
        """ë¹„ë™ê¸° RAG ì‘ë‹µ ìƒì„±.

        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€

        Returns:
            RAG ê¸°ë°˜ ì‘ë‹µ ë¬¸ìì—´
        """
        return await self.chain.ainvoke(message)


class QLoRAService:
    """QLoRA ê¸°ë°˜ ëª¨ë¸ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤.

    PEFTì˜ QLoRA ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì–‘ìí™”í•˜ê³  LoRA ì–´ëŒ‘í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    ëŒ€í™” ë° íŒŒì¸íŠœë‹ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        model_path: str,
        adapter_path: Optional[str] = None,
        device: str = "cuda",
        lora_r: int = 16,
        lora_alpha: int = 32,
        lora_dropout: float = 0.05,
        target_modules: Optional[list[str]] = None,
    ) -> None:
        """QLoRA ì„œë¹„ìŠ¤ ì´ˆê¸°í™”.

        Args:
            model_path: ë² ì´ìŠ¤ ëª¨ë¸ ê²½ë¡œ
            adapter_path: LoRA ì–´ëŒ‘í„° ê²½ë¡œ (Noneì´ë©´ ìƒˆë¡œ í•™ìŠµ)
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (cuda, cpu, auto)
            lora_r: LoRA rank
            lora_alpha: LoRA alpha
            lora_dropout: LoRA dropout
            target_modules: LoRAë¥¼ ì ìš©í•  ëª¨ë“ˆ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ìë™ ê°ì§€)
        """
        self.model_path = model_path
        self.adapter_path = adapter_path
        self.device = device
        self.lora_r = lora_r
        self.lora_alpha = lora_alpha
        self.lora_dropout = lora_dropout
        self.target_modules = target_modules

        self.model: Optional[Any] = None  # AutoModelForCausalLM íƒ€ì… íŒíŠ¸ (ì¡°ê±´ë¶€ import)
        self.tokenizer: Optional[Any] = None  # AutoTokenizer íƒ€ì… íŒíŠ¸ (ì¡°ê±´ë¶€ import)
        self._is_loaded = False

    def _load_model(self) -> None:
        """QLoRA ëª¨ë¸ ë¡œë“œ."""
        if not QLORA_AVAILABLE:
            raise ImportError(
                "QLoRA ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ torch, transformers, peft, datasets íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. "
                "pip install torch transformers peft datasets bitsandbytes"
            )
        if self._is_loaded:
            return

        print(f"ğŸ”„ QLoRA ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_path}")

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if not QLORA_AVAILABLE or torch is None:
            raise ImportError("torchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        if self.device == "auto":
            device_map = "auto"
        elif self.device == "cuda" and torch.cuda.is_available():
            device_map = "cuda:0"
        else:
            device_map = "cpu"

        # 4-bit ì–‘ìí™” ì„¤ì •
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
        )

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            trust_remote_code=False,
        )

        # íŒ¨ë”© í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

        # ëª¨ë¸ ë¡œë“œ (4-bit ì–‘ìí™”)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            quantization_config=bnb_config,
            device_map=device_map,
            trust_remote_code=False,
            torch_dtype=torch.bfloat16,
        )

        # LoRA ì„¤ì •
        if self.target_modules is None:
            # ì¼ë°˜ì ì¸ ëª¨ë¸ êµ¬ì¡°ì— ë§ì¶° ìë™ ì„¤ì •
            self.target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]

        lora_config = LoraConfig(
            r=self.lora_r,
            lora_alpha=self.lora_alpha,
            target_modules=self.target_modules,
            lora_dropout=self.lora_dropout,
            bias="none",
            task_type="CAUSAL_LM",
        )

        # ê¸°ì¡´ ì–´ëŒ‘í„°ê°€ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if self.adapter_path and Path(self.adapter_path).exists():
            print(f"ğŸ“‚ ê¸°ì¡´ LoRA ì–´ëŒ‘í„° ë¡œë“œ: {self.adapter_path}")
            # ì–‘ìí™”ëœ ëª¨ë¸ ì¤€ë¹„
            self.model = prepare_model_for_kbit_training(self.model)
            # ê¸°ì¡´ ì–´ëŒ‘í„° ë¡œë“œ
            self.model = PeftModel.from_pretrained(
                self.model,
                self.adapter_path,
            )
        else:
            # ì–‘ìí™”ëœ ëª¨ë¸ ì¤€ë¹„
            self.model = prepare_model_for_kbit_training(self.model)
            # ìƒˆ LoRA ì–´ëŒ‘í„° ì¶”ê°€
            self.model = get_peft_model(self.model, lora_config)
            print("âœ… ìƒˆ LoRA ì–´ëŒ‘í„° ìƒì„± ì™„ë£Œ")

        self._is_loaded = True
        print(f"âœ… QLoRA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}")

    def chat(self, message: str, max_new_tokens: int = 512, temperature: float = 0.7) -> str:
        """QLoRA ëª¨ë¸ë¡œ ëŒ€í™” ìƒì„±.

        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒì„± ì˜¨ë„

        Returns:
            ìƒì„±ëœ ì‘ë‹µ ë¬¸ìì—´
        """
        if not self._is_loaded:
            self._load_model()

        # í† í°í™”
        inputs = self.tokenizer(
            message,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048,
        )

        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        if not QLORA_AVAILABLE or torch is None:
            raise ImportError("torchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        if self.device == "cuda" and torch.cuda.is_available():
            inputs = {k: v.to("cuda") for k, v in inputs.items()}

        # ìƒì„±
        self.model.eval()
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=True,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        # ë””ì½”ë”© (ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œì™¸í•˜ê³  ìƒì„±ëœ ë¶€ë¶„ë§Œ)
        input_length = inputs["input_ids"].shape[1]
        generated_text = self.tokenizer.decode(
            outputs[0][input_length:],
            skip_special_tokens=True,
        )

        return generated_text.strip()

    async def achat(self, message: str, max_new_tokens: int = 512, temperature: float = 0.7) -> str:
        """ë¹„ë™ê¸° QLoRA ëª¨ë¸ë¡œ ëŒ€í™” ìƒì„±.

        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒì„± ì˜¨ë„

        Returns:
            ìƒì„±ëœ ì‘ë‹µ ë¬¸ìì—´
        """
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            lambda: self.chat(message, max_new_tokens, temperature)
        )

    def train(
        self,
        training_data: list[dict[str, str]],
        output_dir: str,
        num_epochs: int = 3,
        per_device_train_batch_size: int = 4,
        gradient_accumulation_steps: int = 4,
        learning_rate: float = 2e-4,
        warmup_steps: int = 100,
        logging_steps: int = 10,
        save_steps: int = 500,
        max_seq_length: int = 2048,
    ) -> None:
        """QLoRA ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ íŒŒì¸íŠœë‹.

        Args:
            training_data: í•™ìŠµ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ [{"instruction": "...", "input": "...", "output": "..."}]
            output_dir: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            num_epochs: í•™ìŠµ ì—í¬í¬ ìˆ˜
            per_device_train_batch_size: ë””ë°”ì´ìŠ¤ë‹¹ ë°°ì¹˜ í¬ê¸°
            gradient_accumulation_steps: ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…
            learning_rate: í•™ìŠµë¥ 
            warmup_steps: ì›Œë°ì—… ìŠ¤í…
            logging_steps: ë¡œê¹… ê°„ê²©
            save_steps: ì €ì¥ ê°„ê²©
            max_seq_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        """
        if not self._is_loaded:
            self._load_model()

        print(f"ğŸš€ QLoRA íŒŒì¸íŠœë‹ ì‹œì‘...", flush=True)
        print(f"ğŸ“Š í•™ìŠµ ë°ì´í„° ìˆ˜: {len(training_data)}", flush=True)

        # ë°ì´í„° í¬ë§·íŒ…
        def format_prompt(example: dict[str, str]) -> dict[str, str]:
            """í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…."""
            instruction = example.get("instruction", "")
            input_text = example.get("input", "")
            output = example.get("output", "")

            if input_text:
                prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}"
            else:
                prompt = f"### Instruction:\n{instruction}\n\n### Response:\n{output}"

            return {"text": prompt}

        # ë°ì´í„°ì…‹ ìƒì„±
        if not QLORA_AVAILABLE or Dataset is None:
            raise ImportError("datasets íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        dataset = Dataset.from_list(training_data)
        dataset = dataset.map(format_prompt)

        # í† í°í™” í•¨ìˆ˜
        def tokenize_function(examples: dict[str, list[str]]) -> dict[str, list[list[int]]]:
            """í† í°í™” í•¨ìˆ˜."""
            return self.tokenizer(
                examples["text"],
                truncation=True,
                max_length=max_seq_length,
                padding="max_length",
            )

        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names,
        )

        # ë°ì´í„° ì½œë ˆì´í„°
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
        )

        # í•™ìŠµ ì¸ì ì„¤ì •
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=per_device_train_batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            learning_rate=learning_rate,
            warmup_steps=warmup_steps,
            logging_steps=logging_steps,
            save_steps=save_steps,
            save_total_limit=3,
            fp16=True,
            optim="paged_adamw_8bit",
            lr_scheduler_type="cosine",
            report_to="none",
            remove_unused_columns=False,
        )

        # íŠ¸ë ˆì´ë„ˆ ìƒì„±
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
        )

        # í•™ìŠµ ì‹¤í–‰
        print("ğŸ“š í•™ìŠµ ì‹œì‘...")
        trainer.train()

        # ëª¨ë¸ ì €ì¥
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {output_dir}")
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)
        print(f"âœ… í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_dir}")

    def save_adapter(self, adapter_path: str) -> None:
        """LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥.

        Args:
            adapter_path: ì–´ëŒ‘í„° ì €ì¥ ê²½ë¡œ
        """
        if not self._is_loaded:
            raise ValueError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € _load_model()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")

        if not QLORA_AVAILABLE or PeftModel is None:
            raise ImportError("peft íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        if isinstance(self.model, PeftModel):
            self.model.save_pretrained(adapter_path)
            print(f"âœ… LoRA ì–´ëŒ‘í„° ì €ì¥ ì™„ë£Œ: {adapter_path}")
        else:
            raise ValueError("LoRA ì–´ëŒ‘í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    def unload(self) -> None:
        """ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œ."""
        if self.model is not None:
            del self.model
            del self.tokenizer

            if QLORA_AVAILABLE and torch is not None and torch.cuda.is_available():
                torch.cuda.empty_cache()

            self.model = None
            self.tokenizer = None
            self._is_loaded = False
            print("ğŸ—‘ï¸ QLoRA ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
