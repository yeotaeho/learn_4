"""ë¡œì»¬ HuggingFace ëª¨ë¸ ë¡œë” - Mi:dm 2.0 Mini ì§€ì›."""

import asyncio
from typing import Any, Optional

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

from .base import BaseLLM
from .config import LocalModelConfig


class LocalLLM(BaseLLM):
    """ë¡œì»¬ HuggingFace ëª¨ë¸ í´ë˜ìŠ¤.

    Mi:dm 2.0 Mini ë° LlamaForCausalLM ê¸°ë°˜ ëª¨ë¸ ì§€ì›.

    ì‚¬ìš© ì˜ˆì‹œ:
        config = LocalModelConfig(model_path="./model_weights")
        llm = LocalLLM(config)
        llm.load()
        response = llm.generate("ì•ˆë…•í•˜ì„¸ìš”")
    """

    def __init__(self, config: LocalModelConfig) -> None:
        """ë¡œì»¬ LLM ì´ˆê¸°í™”.

        Args:
            config: ë¡œì»¬ ëª¨ë¸ ì„¤ì •
        """
        super().__init__(model_name=config.model_path)
        self.config = config
        self._pipeline: Optional[Any] = None

    def load(self) -> None:
        """ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤."""
        print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {self.config.model_path}")

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if self.config.device == "auto":
            device_map = "auto"
        elif self.config.device == "cuda":
            device_map = "cuda:0"
        else:
            device_map = "cpu"

        # torch dtype ì„¤ì •
        if self.config.torch_dtype == "float16":
            torch_dtype = torch.float16
        elif self.config.torch_dtype == "bfloat16":
            torch_dtype = torch.bfloat16
        else:
            torch_dtype = torch.float32

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self._tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_path,
            trust_remote_code=self.config.trust_remote_code,
        )

        # íŒ¨ë”© í† í° ì„¤ì • (ì—†ìœ¼ë©´ EOS í† í° ì‚¬ìš©)
        if self._tokenizer.pad_token is None:
            self._tokenizer.pad_token = self._tokenizer.eos_token

        # ëª¨ë¸ ë¡œë“œ
        load_kwargs = {
            "pretrained_model_name_or_path": self.config.model_path,
            "trust_remote_code": self.config.trust_remote_code,
            "torch_dtype": torch_dtype,
        }

        # ì–‘ìí™” ì„¤ì •
        if self.config.load_in_4bit:
            from transformers import BitsAndBytesConfig
            load_kwargs["quantization_config"] = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch_dtype,
            )
        elif self.config.load_in_8bit:
            from transformers import BitsAndBytesConfig
            load_kwargs["quantization_config"] = BitsAndBytesConfig(
                load_in_8bit=True,
            )
        else:
            load_kwargs["device_map"] = device_map

        self._model = AutoModelForCausalLM.from_pretrained(**load_kwargs)

        # íŒŒì´í”„ë¼ì¸ ìƒì„±
        self._pipeline = pipeline(
            "text-generation",
            model=self._model,
            tokenizer=self._tokenizer,
            max_new_tokens=self.config.max_tokens,
            do_sample=True,
            temperature=self.config.temperature,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=self._tokenizer.pad_token_id,
        )

        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.config.model_path}")

    def generate(self, prompt: str, **kwargs: Any) -> str:
        """í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            **kwargs: ìƒì„± ì˜µì…˜

        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        if not self.is_loaded:
            self.load()

        # ìƒì„± íŒŒë¼ë¯¸í„° ì˜¤ë²„ë¼ì´ë“œ
        max_new_tokens = kwargs.get("max_tokens", self.config.max_tokens)
        temperature = kwargs.get("temperature", self.config.temperature)

        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        outputs = self._pipeline(
            prompt,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            return_full_text=False,  # í”„ë¡¬í”„íŠ¸ ì œì™¸í•˜ê³  ìƒì„±ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜
        )

        return outputs[0]["generated_text"].strip()

    async def agenerate(self, prompt: str, **kwargs: Any) -> str:
        """ë¹„ë™ê¸°ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            **kwargs: ìƒì„± ì˜µì…˜

        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            lambda: self.generate(prompt, **kwargs)
        )

    def to_langchain(self) -> Any:
        """LangChain í˜¸í™˜ LLM ê°ì²´ë¡œ ë³€í™˜.

        Returns:
            LangChain HuggingFacePipeline ì¸ìŠ¤í„´ìŠ¤
        """
        if not self.is_loaded:
            self.load()

        from langchain_huggingface import HuggingFacePipeline

        return HuggingFacePipeline(pipeline=self._pipeline)

    def unload(self) -> None:
        """ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œí•©ë‹ˆë‹¤."""
        if self._model is not None:
            del self._model
            del self._tokenizer
            del self._pipeline

            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            self._model = None
            self._tokenizer = None
            self._pipeline = None
            print("ğŸ—‘ï¸ ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
