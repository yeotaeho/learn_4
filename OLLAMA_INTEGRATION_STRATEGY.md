# Ollama í†µí•© ì „ëµ

## ğŸ“‹ ëª©ì°¨
1. [ì•„í‚¤í…ì²˜ ê°œìš”](#ì•„í‚¤í…ì²˜-ê°œìš”)
2. [Docker Compose êµ¬ì„±](#docker-compose-êµ¬ì„±)
3. [ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ìˆ˜ì •](#ì• í”Œë¦¬ì¼€ì´ì…˜-ì½”ë“œ-ìˆ˜ì •)
4. [êµ¬í˜„ ë‹¨ê³„](#êµ¬í˜„-ë‹¨ê³„)
5. [ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ](#ëª¨ë¸-ì„ íƒ-ê°€ì´ë“œ)
6. [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)
7. [êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸](#êµ¬í˜„-ì²´í¬ë¦¬ìŠ¤íŠ¸)

---

## ğŸ¯ ì•„í‚¤í…ì²˜ ê°œìš”

### í˜„ì¬ êµ¬ì¡°
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ langchain-app   â”‚â”€â”€â”€â”€â–¶â”‚  postgres    â”‚
â”‚ (Python)        â”‚     â”‚  (pgvector)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ollama ì¶”ê°€ í›„ êµ¬ì¡°
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ langchain-app   â”‚â”€â”€â”€â”€â–¶â”‚  postgres    â”‚
â”‚ (Python + LLM)  â”‚     â”‚  (pgvector)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    ollama       â”‚
â”‚  (LLM Server)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í†µí•© ëª©ì :**
- pgvectorì— ì €ì¥ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰ (Retrieval)
- ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ Ollama LLMì— ì „ë‹¬
- LLMì´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€ (RAG íŒ¨í„´)

---

## ğŸ³ Docker Compose êµ¬ì„±

### Option A: ê¸°ë³¸ Ollama ì„œë¹„ìŠ¤

```yaml
services:
  ollama:
    image: ollama/ollama:latest
    container_name: langchain-ollama
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    networks:
      - langchain-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

volumes:
  postgres_data:
  ollama_data:  # ìƒˆë¡œ ì¶”ê°€
```

### Option B: GPU ì§€ì› (NVIDIA GPU ìˆì„ ê²½ìš°)

```yaml
services:
  ollama:
    image: ollama/ollama:latest
    container_name: langchain-ollama
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    networks:
      - langchain-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
```

### langchain-app ì„œë¹„ìŠ¤ ìˆ˜ì •

```yaml
langchain-app:
  build: .
  container_name: langchain-app
  restart: unless-stopped
  depends_on:
    postgres:
      condition: service_healthy
    ollama:
      condition: service_healthy  # ìƒˆë¡œ ì¶”ê°€
  environment:
    DB_HOST: postgres
    DB_PORT: 5432
    DB_USER: postgres
    DB_PASSWORD: postgres
    DB_NAME: postgres
    OLLAMA_BASE_URL: http://ollama:11434  # ìƒˆë¡œ ì¶”ê°€
  networks:
    - langchain-network
```

---

## ğŸ’» ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ìˆ˜ì •

### 1. requirements.txt ì—…ë°ì´íŠ¸

```txt
langchain-core>=0.1.0
langchain-community>=0.0.20
langchain-ollama>=0.0.1  # ìƒˆë¡œ ì¶”ê°€
psycopg2-binary>=2.9.0
pgvector>=0.2.0
```

### 2. app.py ìˆ˜ì • - Import ì¶”ê°€

```python
"""ê°„ë‹¨í•œ LangChain Hello World ì•± - pgvectorì™€ Ollama ì—°ë™."""

import os
import time
from langchain_core.documents import Document
from langchain_core.embeddings import FakeEmbeddings
from langchain_community.vectorstores import PGVector
from langchain_ollama import OllamaLLM  # ìƒˆë¡œ ì¶”ê°€
from langchain_core.prompts import PromptTemplate  # ìƒˆë¡œ ì¶”ê°€
from langchain_core.output_parsers import StrOutputParser  # ìƒˆë¡œ ì¶”ê°€
from langchain_core.runnables import RunnablePassthrough  # ìƒˆë¡œ ì¶”ê°€
```

### 3. Ollama ì—°ê²° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜

```python
def test_ollama_connection(base_url: str = "http://ollama:11434") -> bool:
    """Ollama ì„œë²„ ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    print("ğŸ” Ollama ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
    max_retries = 30

    for i in range(max_retries):
        try:
            llm = OllamaLLM(
                model="llama2",
                base_url=base_url,
                timeout=10
            )
            response = llm.invoke("Hello")
            print("âœ… Ollama ì—°ê²° ì„±ê³µ!")
            return True
        except Exception as e:
            if i < max_retries - 1:
                print(f"â³ Ollama ì—°ê²° ì‹œë„ {i + 1}/{max_retries}: {str(e)}")
                time.sleep(2)
            else:
                print(f"âŒ Ollama ì—°ê²° ì‹¤íŒ¨: {str(e)}")
                return False

    return False
```

### 4. RAG ì²´ì¸ êµ¬ì„±

```python
def create_rag_chain(vectorstore, llm):
    """RAG (Retrieval-Augmented Generation) ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
    template = """ë‹¤ìŒ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
    ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.

ë¬¸ë§¥: {context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

    prompt = PromptTemplate.from_template(template)

    # Retriever ìƒì„±
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 2}  # ìƒìœ„ 2ê°œ ë¬¸ì„œ ê²€ìƒ‰
    )

    # ë¬¸ì„œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    # RAG ì²´ì¸ êµ¬ì„±
    chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return chain
```

### 5. main í•¨ìˆ˜ì— í†µí•©

```python
def main() -> None:
    """ë©”ì¸ í•¨ìˆ˜: pgvectorì™€ Ollamaë¥¼ ì—°ë™í•˜ì—¬ RAG êµ¬í˜„."""
    print("ğŸš€ LangChain + Ollama Hello World ì•± ì‹œì‘!")

    # í™˜ê²½ë³€ìˆ˜ ì½ê¸°
    ollama_base_url = os.getenv('OLLAMA_BASE_URL', 'http://ollama:11434')

    # PostgreSQL ì—°ê²° (ê¸°ì¡´ ì½”ë“œ)
    # ... vectorstore ìƒì„± ì½”ë“œ ...

    # Ollama ì—°ê²° í…ŒìŠ¤íŠ¸
    if not test_ollama_connection(ollama_base_url):
        print("âš ï¸ Ollama ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
        # Ollama ì—†ì´ ê¸°ë³¸ ë™ì‘ë§Œ ìˆ˜í–‰
        return

    # Ollama LLM ì´ˆê¸°í™”
    print("\nğŸ¤– Ollama LLM ì´ˆê¸°í™” ì¤‘...")
    llm = OllamaLLM(
        model="llama2",  # ë˜ëŠ” mistral, phi3 ë“±
        base_url=ollama_base_url,
        temperature=0.7
    )

    # RAG ì²´ì¸ ìƒì„±
    print("ğŸ”— RAG ì²´ì¸ ìƒì„± ì¤‘...")
    rag_chain = create_rag_chain(vectorstore, llm)

    # RAG í…ŒìŠ¤íŠ¸
    print("\nğŸ§  RAG í…ŒìŠ¤íŠ¸ ì¤‘...")
    question = "ë²¡í„° ê²€ìƒ‰ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"
    print(f"ì§ˆë¬¸: {question}")

    response = rag_chain.invoke(question)
    print(f"\nğŸ¤– LLM ì‘ë‹µ:\n{response}")

    print("\nğŸ‰ RAG ì•± ì‹¤í–‰ ì™„ë£Œ!")
```

---

## ğŸ“ êµ¬í˜„ ë‹¨ê³„

### Phase 1: ê¸°ë³¸ Ollama í†µí•© (30ë¶„)

1. **Docker Compose ìˆ˜ì •**
   - ollama ì„œë¹„ìŠ¤ ì¶”ê°€
   - volumes ì¶”ê°€
   - langchain-appì˜ depends_on ìˆ˜ì •

2. **ì»¨í…Œì´ë„ˆ ì‹œì‘**
   ```bash
   docker-compose up -d
   ```

3. **ëª¨ë¸ ë‹¤ìš´ë¡œë“œ**
   ```bash
   docker exec -it langchain-ollama ollama pull llama2
   # ë˜ëŠ”
   docker exec -it langchain-ollama ollama pull phi3
   ```

4. **ì—°ê²° í…ŒìŠ¤íŠ¸**
   ```bash
   curl http://localhost:11434/api/tags
   ```

### Phase 2: RAG êµ¬í˜„ (1ì‹œê°„)

1. **requirements.txt ì—…ë°ì´íŠ¸**
   - langchain-ollama ì¶”ê°€

2. **app.py ìˆ˜ì •**
   - Import ì¶”ê°€
   - Ollama ì—°ê²° í•¨ìˆ˜ ì¶”ê°€
   - RAG ì²´ì¸ ìƒì„± í•¨ìˆ˜ ì¶”ê°€
   - main í•¨ìˆ˜ ìˆ˜ì •

3. **ì¬ë¹Œë“œ ë° í…ŒìŠ¤íŠ¸**
   ```bash
   docker-compose build langchain-app
   docker-compose up -d
   docker-compose logs -f langchain-app
   ```

### Phase 3: ê³ ë„í™” (ì„ íƒì‚¬í•­)

1. **ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€**
   ```python
   from langchain.memory import ConversationBufferMemory
   ```

2. **ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ**
   ```python
   for chunk in rag_chain.stream(question):
       print(chunk, end="", flush=True)
   ```

3. **ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›**
   ```python
   model_name = os.getenv('OLLAMA_MODEL', 'llama2')
   ```

---

## ğŸ¯ ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

| ëª¨ë¸ | í¬ê¸° | íŠ¹ì§• | RAM ìš”êµ¬ì‚¬í•­ | ì¶”ì²œ ìš©ë„ |
|------|------|------|-------------|-----------|
| **llama2** | 7B | ë²”ìš©, ì•ˆì •ì  | ~8GB | ì¼ë°˜ ëŒ€í™”, QA |
| **llama3** | 8B | ìµœì‹ , ì„±ëŠ¥ í–¥ìƒ | ~8GB | ì¼ë°˜ ëŒ€í™”, QA |
| **mistral** | 7B | ë¹ ë¥¸ ì‘ë‹µ | ~8GB | ë¹ ë¥¸ ì‘ë‹µ í•„ìš” ì‹œ |
| **phi3** | 3.8B | ê²½ëŸ‰, ë¹ ë¦„ | ~4GB | ë¦¬ì†ŒìŠ¤ ì œí•œ í™˜ê²½ |
| **gemma** | 2B-7B | Google ëª¨ë¸ | ~4-8GB | ë‹¤ì–‘í•œ í¬ê¸° ì„ íƒ |
| **codellama** | 7B | ì½”ë“œ íŠ¹í™” | ~8GB | ì½”ë“œ ìƒì„±/ì„¤ëª… |

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ëª…ë ¹ì–´

```bash
# ê¸°ë³¸ ëª¨ë¸
docker exec -it langchain-ollama ollama pull llama2

# ê²½ëŸ‰ ëª¨ë¸ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
docker exec -it langchain-ollama ollama pull phi3

# ìµœì‹  ëª¨ë¸
docker exec -it langchain-ollama ollama pull llama3

# ì½”ë“œ ì „ìš©
docker exec -it langchain-ollama ollama pull codellama
```

### ëª¨ë¸ í™•ì¸

```bash
# ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
docker exec -it langchain-ollama ollama list

# íŠ¹ì • ëª¨ë¸ í…ŒìŠ¤íŠ¸
docker exec -it langchain-ollama ollama run llama2 "Hello!"
```

---

## âš¡ ì„±ëŠ¥ ìµœì í™”

### 1. ë¦¬ì†ŒìŠ¤ ì œí•œ ì„¤ì •

```yaml
ollama:
  deploy:
    resources:
      limits:
        cpus: '4'
        memory: 8G
      reservations:
        cpus: '2'
        memory: 4G
```

### 2. Ollama ì„¤ì • ìµœì í™”

```python
llm = OllamaLLM(
    model="llama2",
    base_url=ollama_base_url,
    temperature=0.7,      # ì°½ì˜ì„± ì¡°ì ˆ (0-1)
    top_k=40,             # ë‹¤ì–‘ì„± ì¡°ì ˆ
    top_p=0.9,            # í™•ë¥  ì„ê³„ê°’
    num_ctx=2048,         # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸°
    repeat_penalty=1.1,   # ë°˜ë³µ ë°©ì§€
)
```

### 3. ìºì‹± ì „ëµ

```python
# ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ìœ ì§€ (keep_alive)
llm = OllamaLLM(
    model="llama2",
    base_url=ollama_base_url,
    keep_alive="5m"  # 5ë¶„ê°„ ë©”ëª¨ë¦¬ ìœ ì§€
)
```

### 4. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```python
def stream_response(chain, question):
    """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‘ë‹µ ì¶œë ¥"""
    print("ğŸ¤– ì‘ë‹µ: ", end="", flush=True)
    for chunk in chain.stream(question):
        print(chunk, end="", flush=True)
    print()
```

---

## ğŸ” ë””ë²„ê¹… ë° ëª¨ë‹ˆí„°ë§

### Ollama ë¡œê·¸ í™•ì¸

```bash
# Ollama ë¡œê·¸
docker logs langchain-ollama

# ì‹¤ì‹œê°„ ë¡œê·¸
docker logs -f langchain-ollama
```

### API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸

```bash
# í—¬ìŠ¤ì²´í¬
curl http://localhost:11434/api/tags

# ëª¨ë¸ ì •ë³´
curl http://localhost:11434/api/show -d '{
  "name": "llama2"
}'

# ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "Why is the sky blue?",
  "stream": false
}'
```

### Python ë””ë²„ê¹… ì½”ë“œ

```python
def debug_ollama():
    """Ollama ìƒíƒœë¥¼ ìì„¸íˆ í™•ì¸"""
    import requests

    base_url = "http://ollama:11434"

    try:
        # 1. ì„œë²„ ìƒíƒœ
        response = requests.get(f"{base_url}/api/tags", timeout=5)
        print(f"âœ… ì„œë²„ ìƒíƒœ: {response.status_code}")
        print(f"ğŸ“‹ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸: {response.json()}")

        # 2. ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸
        response = requests.post(
            f"{base_url}/api/generate",
            json={"model": "llama2", "prompt": "Hi", "stream": False},
            timeout=30
        )
        print(f"âœ… ìƒì„± í…ŒìŠ¤íŠ¸: {response.json()}")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
```

---

## âœ… êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì¤€ë¹„ ë‹¨ê³„
- [ ] ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸ (ìµœì†Œ 8GB RAM ê¶Œì¥)
- [ ] Docker ë° Docker Compose ì„¤ì¹˜ í™•ì¸
- [ ] GPU ì‚¬ìš© ì—¬ë¶€ ê²°ì • (ì„ íƒì‚¬í•­)

### Docker ì„¤ì •
- [ ] `Docker-compose.yaml`ì— ollama ì„œë¹„ìŠ¤ ì¶”ê°€
- [ ] ollama_data ë³¼ë¥¨ ì¶”ê°€
- [ ] langchain-appì˜ depends_onì— ollama ì¶”ê°€
- [ ] í™˜ê²½ë³€ìˆ˜ OLLAMA_BASE_URL ì¶”ê°€
- [ ] í—¬ìŠ¤ì²´í¬ ì„¤ì • ì¶”ê°€

### ì• í”Œë¦¬ì¼€ì´ì…˜ ìˆ˜ì •
- [ ] `requirements.txt`ì— langchain-ollama ì¶”ê°€
- [ ] `app.py`ì— í•„ìš”í•œ import ì¶”ê°€
- [ ] Ollama ì—°ê²° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ì‘ì„±
- [ ] RAG ì²´ì¸ ìƒì„± í•¨ìˆ˜ ì‘ì„±
- [ ] main í•¨ìˆ˜ì— RAG ë¡œì§ í†µí•©

### í…ŒìŠ¤íŠ¸
- [ ] ì»¨í…Œì´ë„ˆ ë¹Œë“œ ë° ì‹œì‘
- [ ] Ollama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (llama2 ë˜ëŠ” phi3)
- [ ] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸ (`ollama list`)
- [ ] ì—°ê²° í…ŒìŠ¤íŠ¸ (curl ë˜ëŠ” Python)
- [ ] RAG ì²´ì¸ ë™ì‘ í™•ì¸
- [ ] ë¡œê·¸ í™•ì¸ ë° ë””ë²„ê¹…

### ìµœì í™” (ì„ íƒì‚¬í•­)
- [ ] ë¦¬ì†ŒìŠ¤ ì œí•œ ì„¤ì •
- [ ] Ollama íŒŒë¼ë¯¸í„° íŠœë‹
- [ ] ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ êµ¬í˜„
- [ ] ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
- [ ] ëª¨ë‹ˆí„°ë§ ì¶”ê°€

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [Ollama ê³µì‹ ë¬¸ì„œ](https://ollama.ai/)
- [LangChain Ollama í†µí•©](https://python.langchain.com/docs/integrations/llms/ollama)
- [LangChain RAG ê°€ì´ë“œ](https://python.langchain.com/docs/use_cases/question_answering/)

### ëª¨ë¸ ì •ë³´
- [Ollama ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬](https://ollama.ai/library)
- [Llama 2 ì •ë³´](https://ai.meta.com/llama/)
- [Mistral AI](https://mistral.ai/)

### Docker ê´€ë ¨
- [Ollama Docker Hub](https://hub.docker.com/r/ollama/ollama)
- [Docker Compose ë¬¸ì„œ](https://docs.docker.com/compose/)

---

## ğŸš€ ì‹œì‘í•˜ê¸°

```bash
# 1. ì»¨í…Œì´ë„ˆ ì‹œì‘
docker-compose up -d

# 2. Ollama ë¡œê·¸ í™•ì¸
docker logs -f langchain-ollama

# 3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ìƒˆ í„°ë¯¸ë„)
docker exec -it langchain-ollama ollama pull llama2

# 4. ì•± ë¡œê·¸ í™•ì¸
docker-compose logs -f langchain-app

# 5. í…ŒìŠ¤íŠ¸
curl http://localhost:11434/api/tags
```

---

**ì‘ì„±ì¼:** 2024-12-16
**ë²„ì „:** 1.0
**ë‹¤ìŒ ë‹¨ê³„:** Phase 1ë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ êµ¬í˜„ ì‹œì‘

